{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create file directories\n",
    "!mkdir data_buffers\n",
    "!mkdir models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile featurise_phc_dataset.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import phclib as pp\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print('Generating features....')\n",
    "start = pd.Timestamp.now()\n",
    "\n",
    "# Load dataset\n",
    "\n",
    "df = pd.read_parquet('phc_data.pqt')\n",
    "weather_internal = pd.read_parquet('weather_data.pqt')\n",
    "\n",
    "# Convert to daily format to append daily data such as fast-logging, weather or other (here we just use weather)\n",
    "\n",
    "meters_list = pp.chunks(df.meterref.unique(), 35) # Break the data into chunks for processing to fit into memory\n",
    "\n",
    "df_list = []\n",
    "\n",
    "for i, meters in enumerate(meters_list):\n",
    "    print('{}/{}   '.format(i+1,len(meters_list)), end='\\r')\n",
    "    df_subset = df[df.meterref.isin(meters)].copy()\n",
    "\n",
    "    # Create a multiplier dataframe in order to set up to perform approximate merge on it\n",
    "\n",
    "    multiplied_weather_df_list = []\n",
    "    for meter in df_subset['meterref'].unique():\n",
    "        df_part = weather_internal[['dateofread_dt']].copy()\n",
    "        df_part['meterref'] = meter\n",
    "        multiplied_weather_df_list.append(df_part)\n",
    "    multiplied_weather_df = pd.concat(multiplied_weather_df_list)\n",
    "    del multiplied_weather_df_list\n",
    "\n",
    "    # Do a backwards approximate merge to associate any days of weather data to appropriate meter window\n",
    "    combined_df = pd.merge_asof(multiplied_weather_df.sort_values('dateofread_dt'),\n",
    "                                df_subset[['prevreaddate', 'meterref','currreaddate','daily_consumption', 'prev_daily_consumption']].sort_values('prevreaddate'),\n",
    "                                by='meterref',\n",
    "                                left_on='dateofread_dt',\n",
    "                                right_on='prevreaddate',\n",
    "                                direction='backward')\n",
    "\n",
    "    # Remove any data outside the meter read window\n",
    "    combined_df = combined_df[(combined_df.prevreaddate >= combined_df.dateofread_dt.min()) & (combined_df.currreaddate > combined_df.dateofread_dt)]\n",
    "    combined_df = combined_df.drop_duplicates(['meterref','prevreaddate','dateofread_dt'])\n",
    "    \n",
    "    combined_df = combined_df.sort_values(['meterref','dateofread_dt']).reset_index(drop=True)\n",
    "\n",
    "    combined_df['cum_prev_daily_consumption2'] = combined_df.groupby('meterref')['prev_daily_consumption'].cumsum() / combined_df.groupby('meterref')['prev_daily_consumption'].cumcount()\n",
    "\n",
    "    # Merge in weather data\n",
    "    combined_df = combined_df.merge(weather_internal, how='left',on='dateofread_dt')\n",
    "\n",
    "    # Save daily data buffers to be used later\n",
    "    combined_df[['prevreaddate', 'meterref','dateofread_dt','currreaddate','daily_consumption', 'cum_prev_daily_consumption2']].to_parquet(os.getcwd()+'/data_buffers/daily_subset_part_{}'.format(i))\n",
    "\n",
    "    # Create a variable for each date\n",
    "    for date in combined_df['dateofread_dt'].unique():\n",
    "        combined_df['{}'.format(date)] =  (combined_df['dateofread_dt'] == date).astype(np.uint8)\n",
    "\n",
    "    #Remove fields no longer needed, as they are present in the original data subset\n",
    "    combined_df.drop(['daily_consumption','currreaddate', 'prev_daily_consumption'],1,inplace=True)\n",
    "\n",
    "    # Aggregate back to the meter read window\n",
    "    combined_df = pp.compress_df(combined_df)\n",
    "    agg_dict = {**{feat:'mean' for feat in weather_internal.drop('dateofread_dt',1)} ,\n",
    "                **{feat: 'sum' for feat in [c for c in combined_df if '20' in c]},\n",
    "                **{'cum_prev_daily_consumption2': 'mean'}}\n",
    "\n",
    "    combined_df_agg = combined_df.groupby(['meterref','prevreaddate']).agg(agg_dict).reset_index()\n",
    "\n",
    "    # Merge in the rest of the original data\n",
    "    df_subset = df_subset.merge(combined_df_agg, on=['meterref','prevreaddate'])\n",
    "    df_list.append(df_subset)\n",
    "\n",
    "final = pd.concat(df_list)\n",
    "del df_list\n",
    "\n",
    "# Save meterref breakdown for future use\n",
    "pp.save_obj(meters_list, 'meterref_split_list')\n",
    "    \n",
    "\n",
    "# One-hot-encode categoricals\n",
    "for c in ['b_class','age','type','storeys','classdesc','primarydesc','secondarydesc',\n",
    "          'tertiarydesc','quaternarydesc','acorntype', 'acorncat', 'acorngroup']:\n",
    "    final = pd.concat([final, pd.get_dummies(final[c], prefix=c)], 1)\n",
    "    final.drop(c, 1, inplace=True)\n",
    "\n",
    "final = final.sort_values(['meterref','prevreaddate'])\n",
    "final.columns = pp.pythonify_cols(final.columns)\n",
    "    \n",
    "# Apply exclusions\n",
    "final = final[(final['reading_lag_prev'] >= 0) & (final['reading_lag_prev'] < 90)] # to bridge gap of small missing reading windows\n",
    "final = final[final['reading_lag_prev_start'] <= 200] # Select cases where previous reading window was not too long ago\n",
    "final = final[final['reading_window_days'] < 200] # Use under 5 month intervals only in order to get variance within the year\n",
    "final = final[(final['daily_consumption'] >= 0) & (final['daily_consumption'] < 200)] # Ensure no 99999 and other corruped readings\n",
    "final = final[(final['prev_daily_consumption'] >= 0) & (final['prev_daily_consumption'] < 200)]# Ensure no 99999 and other corruped readings\n",
    "\n",
    "#Save data\n",
    "final.to_parquet('featurised_phc_data.pqt')\n",
    "\n",
    "print('Finished featurisation. Data shape: {} / Time elapsed (minutes): {}'.format(final.shape, (pd.Timestamp.now() - start) / pd.Timedelta(1, 'm')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile build_daily_variance_models.py\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import phclib as pp\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print('Training models...')\n",
    "start = pd.Timestamp.now()\n",
    "\n",
    "# Load relevant data\n",
    "weather_internal = pd.read_parquet('weather_data.pqt', columns=['dateofread_dt'])\n",
    "model_data = pd.read_parquet('featurised_phc_data.pqt')\n",
    "model_data = model_data[(model_data.prevreaddate >= '2018-02-01 00:00:00')]\n",
    "\n",
    "# Decide on modelling window\n",
    "modelling_window_dates = weather_internal[(weather_internal.dateofread_dt >= '2018-03-01 00:00:00') & (weather_internal.dateofread_dt < '2020-07-29 00:00:00')].dateofread_dt.unique()\n",
    "modelling_window_dates = pp.pythonify_cols(modelling_window_dates.astype(str))\n",
    "\n",
    "# Exclude features not to model on - the idea is to only get variance based on seasonality, weather and static data\n",
    "exclude = ['prevreaddate','meterref','dateofread_dt','currreaddate','daily_consumption',\n",
    "          'reading_window_days', 'reading_lag_prev_start'] + pp.pythonify_cols([x.lower() for x in model_data if '000000000' in x])\n",
    "\n",
    "# Train models in a loop\n",
    "for i, date in enumerate(modelling_window_dates):\n",
    "    \n",
    "    model_data_subset = model_data[model_data[str(date)] >= 1].copy()\n",
    "    print('Model number, date and data shape: ', i+1, date, model_data_subset.shape[0], end='\\r')\n",
    "    use_cols = [c for c in model_data_subset if c not in exclude]\n",
    "\n",
    "    model_data_subset = model_data_subset.reset_index(drop=True)\n",
    "\n",
    "    # Create train / test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(model_data_subset[use_cols], model_data_subset['daily_consumption'],\n",
    "                                                        test_size=0.1, random_state=1)\n",
    "\n",
    "    # Initiate xgb model with parameters that are likely to overfit as we'll be using an automated stopping point\n",
    "    XGB_model = XGBRegressor(learning_rate=0.075,\n",
    "                             max_depth=5,\n",
    "                             n_estimators=250,\n",
    "                             n_jobs=31,\n",
    "                             objective='reg:squarederror')\n",
    "    \n",
    "    # Fit with automated stopping based on the test set\n",
    "    XGB_model.fit(X_train,\n",
    "                  y_train,\n",
    "                  verbose=False,\n",
    "                  early_stopping_rounds=10,\n",
    "                  eval_metric='rmse',\n",
    "                  eval_set=[(X_test, y_test)])\n",
    "    \n",
    "    \n",
    "    train_score = XGB_model.score(X_train, y_train)\n",
    "    test_score = XGB_model.score(X_test, y_test)\n",
    "\n",
    "    model_data_subset['predicted'] = XGB_model.predict(model_data_subset[use_cols])\n",
    "    total_pred = model_data_subset['predicted'].sum()\n",
    "    total_act = model_data_subset['daily_consumption'].sum()\n",
    "    sample_shape = model_data_subset.shape[0]\n",
    "    res = {date: {'model': XGB_model,\n",
    "                           'train_score': train_score, \n",
    "                           'test_score': test_score,\n",
    "                           'total_pred': total_pred,\n",
    "                           'total_actual': total_act,\n",
    "                           'sample_shape': sample_shape\n",
    "                          }}\n",
    "\n",
    "    pp.save_obj(res, os.getcwd()+'/models/model_{}.pkl'.format(date), full_path=True)\n",
    "# pp.save_obj(res, os.getcwd()+'/fall_back_model_{}.pkl'.format(date), full_path=True)\n",
    "print('Saving to disk. Successfully built {} models over {} minutes'.format(len(modelling_window_dates), (pd.Timestamp.now() - start) / pd.Timedelta(1, 'm')))\n",
    "pp.save_obj(use_cols, 'all_feats')\n",
    "pp.save_obj(modelling_window_dates, 'modelling_window')\n",
    "\n",
    "\n",
    "stats = [list(pp.load_obj(os.getcwd()+'/models/{}'.format(file), full_path=True).values())[0]['train_score'] for file in os.listdir(os.getcwd()+'/models/')]\n",
    "print('Train Score: ', np.mean(stats))\n",
    "\n",
    "stats = [list(pp.load_obj(os.getcwd()+'/models/{}'.format(file), full_path=True).values())[0]['test_score'] for file in os.listdir(os.getcwd()+'/models/')]\n",
    "print('Test Score: ', np.mean(stats))\n",
    "\n",
    "stats = [list(pp.load_obj(os.getcwd()+'/models/{}'.format(file), full_path=True).values())[0]['total_pred'] for file in os.listdir(os.getcwd()+'/models/')]\n",
    "print('Total prediction: ', np.mean(stats))\n",
    "\n",
    "stats = [list(pp.load_obj(os.getcwd()+'/models/{}'.format(file), full_path=True).values())[0]['total_actual'] for file in os.listdir(os.getcwd()+'/models/')]\n",
    "print('Total actual: ', np.mean(stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile daily_redistribution_plus_featurisation.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import phclib as pp\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print('Redistributing the dependent variable (daily consumption) based on the daily models...')\n",
    "start = pd.Timestamp.now()\n",
    "\n",
    "# Load files\n",
    "meters_list = pp.load_obj('meterref_split_list')\n",
    "df = pd.read_parquet('phc_data.pqt')\n",
    "all_feats = pp.load_obj('all_feats')\n",
    "modelling_window_dates = pp.load_obj('modelling_window')\n",
    "modelling_window_dates_date_format = pd.to_datetime(pd.Series(modelling_window_dates).str.replace('_','-')).to_list()\n",
    "weather_internal = pd.read_parquet('weather_data.pqt')\n",
    "\n",
    "# Load the buffered daily data from before and merge in all the rest of the data\n",
    "for i, meters in enumerate(meters_list):\n",
    "    print('{}/{}{}'.format(i+1,len(meters_list), ' '*20), end='\\r')\n",
    "    df_subset = df[df.meterref.isin(meters)].copy()\n",
    "    if df_subset.shape[0] == 0:\n",
    "        continue\n",
    "    combined_df = pd.read_parquet(os.getcwd()+'/data_buffers/daily_subset_part_{}'.format(i))\n",
    "    combined_df.drop(['daily_consumption','currreaddate'],1,inplace=True)\n",
    "    \n",
    "    # Add primary dataset features\n",
    "    combined_df = combined_df.merge(df_subset, on=['meterref','prevreaddate'], how='left')\n",
    "    \n",
    "    # Apply exclusions\n",
    "    combined_df = combined_df[(combined_df['reading_lag_prev'] >= 0) & (combined_df['reading_lag_prev'] < 90)] # to bridge gap of small missing reading windows\n",
    "    combined_df = combined_df[combined_df['reading_lag_prev_start'] <= 200] # Select cases where previous reading window was not too long ago\n",
    "    combined_df = combined_df[(combined_df['daily_consumption'] >= 0) & (combined_df['daily_consumption'] < 200)] # Ensure no 99999 and other corruped readings\n",
    "    combined_df = combined_df[(combined_df['prev_daily_consumption'] >= 0) & (combined_df['prev_daily_consumption'] < 200)]# Ensure no 99999 and other corruped readings\n",
    "    \n",
    "    # Add weather features\n",
    "    combined_df = combined_df.merge(weather_internal, how='left',on='dateofread_dt')\n",
    "    \n",
    "    # Add static features\n",
    "    for c in ['b_class','age','type','storeys','classdesc','primarydesc','secondarydesc','tertiarydesc',\n",
    "              'quaternarydesc','acorntype', 'acorncat', 'acorngroup']:\n",
    "        combined_df = pd.concat([combined_df, pd.get_dummies(combined_df[c], prefix=c)], 1)\n",
    "        combined_df.drop(c, 1, inplace=True)\n",
    "        \n",
    "    combined_df.columns = pp.pythonify_cols(combined_df.columns)\n",
    "        \n",
    "    # Ensure all variable names are in each chunk of data. Use with caution.\n",
    "    for feat in all_feats:\n",
    "        if feat not in combined_df:\n",
    "            combined_df[feat] = 0\n",
    "    \n",
    "    # Predict new daily values based on the models built\n",
    "    combined_df_list = []\n",
    "    for date, date_ in zip(modelling_window_dates, modelling_window_dates_date_format):\n",
    "        print('{}   '.format(date), end='\\r')\n",
    "        combined_df_subset = combined_df[combined_df.dateofread_dt == date_].copy()\n",
    "        model = pp.load_obj(os.getcwd()+'/models/model_{}.pkl'.format(date), full_path=True)[date]['model']\n",
    "        combined_df_subset['predicted'] = model.predict(combined_df_subset[model.get_booster().feature_names])\n",
    "        combined_df_list.append(combined_df_subset)\n",
    "    combined_df = pd.concat(combined_df_list)\n",
    "    \n",
    "    # Redistribute the actual meter read values according to the daily predicted values distribution and remove negative xgb residuals\n",
    "    temp = combined_df[['meterref','prevreaddate','daily_consumption', 'dateofread_dt', 'predicted']].copy()\n",
    "    temp['above_average'] = temp['predicted'] > temp['daily_consumption']\n",
    "    temp['negative'] = temp['predicted'] < 0\n",
    "    temp = temp.merge(temp[temp['negative']].groupby(['meterref','prevreaddate'])['predicted'].min().rename('neg_min').astype(np.float32).reset_index(), how='left')\n",
    "    temp['predicted'] += temp['neg_min'].fillna(0).abs()\n",
    "    temp = temp.merge(temp.groupby(['meterref', 'prevreaddate'])['daily_consumption'].sum().rename('total_consumption').reset_index(), how='left')\n",
    "    temp = temp.merge(temp.groupby(['meterref', 'prevreaddate'])['predicted'].sum().rename('pred_sum').reset_index(), how='left')\n",
    "    temp['pred_distribution'] = temp['predicted'] / temp['pred_sum']\n",
    "    temp['new_daily_consumption'] = temp['total_consumption'] * temp['pred_distribution']\n",
    "    combined_df = combined_df.merge(temp[['meterref','prevreaddate','new_daily_consumption', 'dateofread_dt']], how='left',on=['meterref','prevreaddate', 'dateofread_dt'])\n",
    "    del temp\n",
    "\n",
    "    combined_df.to_parquet(os.getcwd()+'/data_buffers/daily_adjusted_subset_part_{}'.format(i))\n",
    "    \n",
    "print('Finished featurisation. Time elapsed: {}{}'.format((pd.Timestamp.now() - start) / pd.Timedelta(1, 'm'), ' '*20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile generate_final_dataset.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import phclib as pp\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print('Merging files and generating final subsampled dataset...')\n",
    "start = pd.Timestamp.now()\n",
    "\n",
    "meters_list = pp.load_obj('meterref_split_list')\n",
    "sample_days_per_month = 5\n",
    "pre_covid_frac = 1\n",
    "\n",
    "# Due to limited data since the beginning of COVID19 we subsample from pre-covid period only in order to fit data into memory\n",
    "for i, meters in enumerate(meters_list):\n",
    "    print('{}/{}   '.format(i+1,len(meters_list)), end='\\r')\n",
    "    if i == 0:\n",
    "        df_daily = pd.read_parquet(os.getcwd()+'/data_buffers/daily_adjusted_subset_part_{}'.format(i)).drop(['currreaddate'], 1)\n",
    "        \n",
    "        # Sample 5 random days per meter per month\n",
    "        df_daily['month'] = df_daily['dateofread_dt'].dt.month.astype('int16')\n",
    "        df_daily = df_daily.sample(frac=1)\n",
    "        df_daily = df_daily.groupby(['meterref','month']).head(5)\n",
    "        df_daily.drop('month',1, inplace=True)\n",
    "        \n",
    "        # All the ints are dummy variables so we reduce memory requirements\n",
    "        for feat in df_daily.select_dtypes(include=['int64', 'int32', 'int16', 'int8']):\n",
    "            df_daily[feat] = df_daily[feat].astype(np.uint8)\n",
    "        df_daily = pp.compress_df(df_daily)\n",
    "        \n",
    "        # Split data to pre- and post-isolation in order to retain the data at that time\n",
    "        df_daily_post_mar_2020 = df_daily[df_daily.dateofread_dt >= '2020-03-18']\n",
    "        covid_data = df_daily_post_mar_2020[df_daily_post_mar_2020.dateofread_dt >= '2020-06-01']['meterref'].unique()\n",
    "        df_daily_pre_mar_2020 = df_daily[df_daily.dateofread_dt < '2020-03-18']\n",
    "        df_daily_pre_mar_2020_covid = df_daily_pre_mar_2020[df_daily_pre_mar_2020['meterref'].isin(covid_data)]\n",
    "        df_daily_pre_mar_2020_ncovid = df_daily_pre_mar_2020[~df_daily_pre_mar_2020['meterref'].isin(covid_data)]\n",
    "        df_daily = pd.concat([df_daily_post_mar_2020, df_daily_pre_mar_2020_covid, df_daily_pre_mar_2020_ncovid.sample(frac=pre_covid_frac)])\n",
    "        \n",
    "        del df_daily_post_mar_2020, df_daily_pre_mar_2020\n",
    "    else:\n",
    "        df_daily_temp = pd.read_parquet(os.getcwd()+'/data_buffers/daily_adjusted_subset_part_{}'.format(i)).drop(['currreaddate'], 1)\n",
    "        \n",
    "        # Sample 5 random days per meter per month\n",
    "        df_daily_temp['month'] = df_daily_temp['dateofread_dt'].dt.month.astype('int16')\n",
    "        df_daily_temp = df_daily_temp.sample(frac=1)\n",
    "        df_daily_temp = df_daily_temp.groupby(['meterref','month']).head(5)\n",
    "        df_daily_temp.drop('month',1, inplace=True)\n",
    "        \n",
    "        \n",
    "        # All the ints are dummy variables so we reduce memory requirements\n",
    "        for feat in df_daily_temp.select_dtypes(include=['int64', 'int32', 'int16', 'int8']):\n",
    "            df_daily_temp[feat] = df_daily_temp[feat].astype(np.uint8)\n",
    "            \n",
    "        # Split data to pre- and post-isolation in order to retain the data at that time\n",
    "        df_daily_temp = pp.compress_df(df_daily_temp)\n",
    "        df_daily_temp_post_mar_2020 = df_daily_temp[df_daily_temp.dateofread_dt >= '2020-03-18']\n",
    "        df_daily_temp_pre_mar_2020 = df_daily_temp[df_daily_temp.dateofread_dt < '2020-03-18']\n",
    "        covid_data = df_daily_temp_post_mar_2020[df_daily_temp_post_mar_2020.dateofread_dt >= '2020-06-01']['meterref'].unique()\n",
    "        df_daily_pre_mar_2020_covid = df_daily_temp_pre_mar_2020[df_daily_temp_pre_mar_2020['meterref'].isin(covid_data)]\n",
    "        df_daily_pre_mar_2020_ncovid = df_daily_temp_pre_mar_2020[~df_daily_temp_pre_mar_2020['meterref'].isin(covid_data)]\n",
    "        df_daily_temp = pd.concat([df_daily_temp_post_mar_2020, df_daily_pre_mar_2020_covid, df_daily_pre_mar_2020_ncovid.sample(frac=pre_covid_frac)])\n",
    "        \n",
    "        df_daily = pd.concat([df_daily, df_daily_temp])\n",
    "        del df_daily_temp_post_mar_2020, df_daily_temp_pre_mar_2020, df_daily_temp,df_daily_pre_mar_2020_covid,df_daily_pre_mar_2020_ncovid\n",
    "        \n",
    "# Add seasonality and covid-specific features\n",
    "misc_variables = pp.compress_df(pd.read_parquet('misc_data_preprocessed.pqt'))\n",
    "df_daily = df_daily.merge(misc_variables, how='left',on='dateofread_dt')\n",
    "df_daily['month'] = df_daily['dateofread_dt'].dt.month\n",
    "df_daily = pd.concat([df_daily, pd.get_dummies(df_daily['month'], prefix='month')], 1).drop('month',1)\n",
    "\n",
    "# Add rolling weather variables to identify persisting weather trends\n",
    "weather_internal = pd.read_parquet('weather_data.pqt').sort_values('dateofread_dt')\n",
    "for c in weather_internal.drop('dateofread_dt',1):\n",
    "    weather_internal['{}_rolling7'.format(c)] = weather_internal[c].rolling(7).mean().bfill().reset_index(drop=True).astype(np.float32)\n",
    "    weather_internal['{}_rolling21'.format(c)] = weather_internal[c].rolling(21).mean().bfill().reset_index(drop=True).astype(np.float32)\n",
    "    weather_internal.drop(c, 1, inplace=True)\n",
    "    \n",
    "df_daily = df_daily.merge(weather_internal, how='left', on='dateofread_dt')\n",
    "        \n",
    "print('Saving to disk. Data shape: {} / Time Elapsed: {} minutes{}'.format(df_daily.shape, (pd.Timestamp.now() - start) / pd.Timedelta(1, 'm'), ' '*20))\n",
    "df_daily.to_parquet('full_daily_pcc_dataset.pqt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile general_daily_model.py\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import phclib as pp\n",
    "import pickle\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Build general model without recent consumption figures in order to be able to separate out COVID impact\n",
    "\n",
    "print('Training general model on the final daily dataset')\n",
    "start = pd.Timestamp.now()\n",
    "\n",
    "# Load relevant data\n",
    "model_data = pd.read_parquet('full_daily_pcc_dataset.pqt')\n",
    "\n",
    "# Downsample to about 20% to fit into memory\n",
    "\n",
    "model_data['month'] = model_data['dateofread_dt'].dt.month.astype('int16')\n",
    "model_data = model_data.sample(frac=1)\n",
    "model_data = model_data.groupby(['meterref','month']).head(1)\n",
    "model_data.drop('month',1, inplace=True)\n",
    "model_data = model_data[model_data.dateofread_dt < '2020-02-01 00:00:00']\n",
    "\n",
    "model_data['new_daily_consumption'] = model_data['new_daily_consumption'].fillna(0)\n",
    "\n",
    "print('Modelling data shape: ', model_data.shape)\n",
    "\n",
    "exclude = ['prevreaddate','meterref','dateofread_dt','currreaddate','daily_consumption',\n",
    "           'new_daily_consumption', 'reading_window_days', 'reading_lag_prev_start','predicted',\n",
    "           'prev_daily_consumption', 'cum_prev_daily_consumption', 'cum_prev_daily_consumption2']\n",
    "use_cols = [c for c in model_data if c not in exclude]\n",
    "\n",
    "# Create train / test split\n",
    "X_train, X_test, y_train, y_test = pp.train_test_split_by_key(model_data,\n",
    "                                                           target='new_daily_consumption',\n",
    "                                                           usecols=use_cols,\n",
    "                                                           key='meterref',\n",
    "                                                           test_size=0.5)\n",
    "\n",
    "del model_data\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "XGB_model = XGBRegressor(learning_rate=0.05,\n",
    "                         max_depth=5,\n",
    "                         n_estimators=500,\n",
    "                         n_jobs=31,\n",
    "                         objective='reg:squarederror')\n",
    "\n",
    "# Fit with automated stopping based on the test set\n",
    "print('Fit start')\n",
    "print(X_train.shape)\n",
    "XGB_model.fit(X_train,\n",
    "              y_train,\n",
    "              verbose=True,\n",
    "              early_stopping_rounds=20,\n",
    "              eval_metric='rmse',\n",
    "              eval_set=[(X_test, y_test)])\n",
    "\n",
    "train_score = XGB_model.score(X_train, y_train)\n",
    "test_score = XGB_model.score(X_test, y_test)\n",
    "print('Train score: {} \\nTest score: {}'.format(train_score, test_score))\n",
    "\n",
    "print('Saving to disk. Time Elapsed: {}'.format((pd.Timestamp.now() - start) / pd.Timedelta(1, 'm')))\n",
    "pp.save_obj(XGB_model, 'raw_daily_xgb_pcc_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile redistribute_noncovid_consumption.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import phclib as pp\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print('Redistributing consumption after covid impact removal...')\n",
    "start = pd.Timestamp.now()\n",
    "\n",
    "# Remove covid impact by predicting with nullified covid variables and applying the percentage change to the daily consumption\n",
    "\n",
    "# Load data and downsample\n",
    "model_data = pd.read_parquet('full_daily_pcc_dataset.pqt')\n",
    "model_data['month'] = model_data['dateofread_dt'].dt.month.astype('int16')\n",
    "model_data = model_data.sample(frac=1)\n",
    "model_data = model_data.groupby(['meterref','month']).head(1)\n",
    "model_data.drop('month',1, inplace=True)\n",
    "\n",
    "XGB_model = pp.load_obj('raw_daily_xgb_pcc_model')\n",
    "misc_variables = pp.compress_df(pd.read_parquet('misc_data_preprocessed.pqt')).columns\n",
    "\n",
    "# Predict based on trained model\n",
    "model_data['predicted'] = XGB_model.predict(model_data[XGB_model.get_booster().feature_names])\n",
    "\n",
    "# Nullify covid variables\n",
    "for c in ['new_covid_deaths','new_hospital_admissions','new_covid_cases']:\n",
    "    model_data[c] = np.NaN\n",
    "\n",
    "for c in ['is_covid_period'] + [c for c in misc_variables if 'covid_lockdown_stage' in c]:\n",
    "    model_data[c] = 0\n",
    "    \n",
    "model_data['predicted_noncovid'] = XGB_model.predict(model_data[XGB_model.get_booster().feature_names])\n",
    "\n",
    "# Generate covid impact stats\n",
    "cvd_meters = model_data[(model_data['dateofread_dt'] >= '2020-06-01 00:00:00')].meterref.unique()\n",
    "cvd_set = model_data[(model_data['dateofread_dt'] >= '2020-03-01 00:00:00') & model_data['meterref'].isin(cvd_meters)]\n",
    "cvd_impact = (cvd_set['predicted'].sum() - cvd_set['predicted_noncovid'].sum()) / cvd_set['predicted_noncovid'].sum()\n",
    "pp.save_obj(cvd_impact, 'covid_impact_percentage')\n",
    "\n",
    "print(\"Covid impact: \", cvd_impact)\n",
    "\n",
    "model_data['cvd_impact'] = model_data['predicted_noncovid'] / model_data['predicted']\n",
    "model_data.loc[model_data['dateofread_dt'] >= '2020-03-01 00:00:00', 'new_daily_consumption'] = model_data.loc[model_data['dateofread_dt'] >= '2020-03-01 00:00:00', 'new_daily_consumption'] * model_data.loc[model_data['dateofread_dt'] >= '2020-03-01 00:00:00', 'cvd_impact']\n",
    "model_data.drop(['predicted','predicted_noncovid','cvd_impact'], 1).to_parquet('full_daily_pcc_dataset_noncovid.pqt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile final_model.py\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost\n",
    "import shap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import phclib as pp\n",
    "import pickle\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print('Training final clean model for ongoing phc calculations...')\n",
    "start = pd.Timestamp.now()\n",
    "\n",
    "# Build final model without the impact of covid\n",
    "\n",
    "# Load relevant data\n",
    "model_data = pd.read_parquet('full_daily_pcc_dataset_noncovid.pqt').sample(frac=0.75)\n",
    "model_data['new_daily_consumption'] = model_data['new_daily_consumption'].fillna(0)\n",
    "\n",
    "misc_variables_cols = pd.read_parquet('misc_data_preprocessed.pqt').columns\n",
    "\n",
    "# Exclude COVID-related features apart from normal exclusions\n",
    "exclude = ['prevreaddate','meterref','dateofread_dt','currreaddate','daily_consumption', 'new_daily_consumption', 'reading_window_days']\n",
    "exclude.extend(['new_covid_deaths','new_hospital_admissions','new_covid_cases'])\n",
    "exclude.extend(['is_covid_period'] + [c for c in misc_variables_cols if 'covid_lockdown_stage' in c])\n",
    "model_data.columns = pp.pythonify_cols(model_data.columns)\n",
    "use_cols = [c for c in model_data if c not in exclude]\n",
    "\n",
    "# Create train / test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(model_data[use_cols], model_data['new_daily_consumption'],\n",
    "                                                    test_size=0.5, random_state=1)\n",
    "\n",
    "# Free up memory\n",
    "del model_data\n",
    "\n",
    "\n",
    "# Train model\n",
    "XGB_model = XGBRegressor(learning_rate=0.05,\n",
    "                         max_depth=5,\n",
    "                         n_estimators=750,\n",
    "                         n_jobs=31,\n",
    "                         objective='reg:squarederror')\n",
    "\n",
    "XGB_model.fit(X_train,\n",
    "              y_train,\n",
    "              verbose=True,\n",
    "              early_stopping_rounds=20,\n",
    "              eval_metric='rmse',\n",
    "              eval_set=[(X_test, y_test)])\n",
    "\n",
    "train_score = XGB_model.score(X_train, y_train)\n",
    "test_score = XGB_model.score(X_test, y_test)\n",
    "\n",
    "print('Train score: {} \\nTest score: {}'.format(train_score, test_score))\n",
    "print('Saving to disk. Time Elapsed: {}'.format((pd.Timestamp.now() - start) / pd.Timedelta(1, 'm')))\n",
    "\n",
    "pp.save_obj(XGB_model, 'daily_xgb_phc_model')\n",
    "\n",
    "print('\\nGenerating model validation...\\n')\n",
    "print('1. Permutation Feature Importance (see eli5 documentation).')\n",
    "print('2. Shapley scores feature importance (Efficiency, Symmetry, Dummy and Additivity) and effect on model outputs.')\n",
    "\n",
    "#Xgboost feature importance\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 30))\n",
    "xgboost.plot_importance(XGB_model, max_num_features=100, ax=ax)\n",
    "plt.savefig('feat_importances.png')\n",
    "\n",
    "# Shap scores model interpretation\n",
    "shap_x_train = X_train[XGB_model.get_booster().feature_names].sample(frac=0.5)\n",
    "\n",
    "# Create our SHAP explainer\n",
    "shap_explainer = shap.TreeExplainer(XGB_model)\n",
    "\n",
    "# Calculate the shapley values for our data\n",
    "shap_values = shap_explainer.shap_values(shap_x_train)\n",
    "shap.summary_plot(shap_values, shap_x_train,auto_size_plot=True)\n",
    "plt.savefig('shap_summary.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "(Python) asen_science1",
   "language": "python",
   "name": "asen_science1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
